{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from keras.models import load_model, Model, Sequential\n",
    "from keras.optimizers import RMSprop, Adam, SGD\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.activations import relu\n",
    "\n",
    "from keras.layers import Input, Activation, Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2DTranspose, UpSampling2D\n",
    "from keras.layers.core import Reshape, Dropout\n",
    "from keras.layers.core import Lambda\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageOps\n",
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "from ImageProcessing import get_viewport\n",
    "import numpy as np\n",
    "\n",
    "from CameraOperations import show_grid\n",
    "\n",
    "from fcnprocessing import ImageDataGenerator\n",
    "import keras\n",
    "\n",
    "\n",
    "def run_prediction(x):\n",
    "    prediction = model.predict(x, 1, True)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def read_config(config_file=\"./imagenet1000_clsid_to_human_array.json\"):\n",
    "    '''\n",
    "      Read Configuration parameters from external file\n",
    "    '''\n",
    "    import json\n",
    "    \n",
    "    with open(config_file) as config_data:\n",
    "        config = json.load(config_data)\n",
    "    return config\n",
    "\n",
    "\n",
    "def predict_with_mobilenet(image, model=None):\n",
    "    if model is None:\n",
    "        model = load_model('./mobilenet_1_0_224_tf_local.h5') \n",
    "    image_input = image/255.\n",
    "    X = np.expand_dims(image_input, 0)\n",
    "    prediction = model.predict(X, len(X), False)\n",
    "    idx = np.argmax(prediction, 1)[0]\n",
    "    return idx, prediction, model\n",
    "\n",
    "def save_channels(prediction, classes=1000):\n",
    "    max_prob = np.max(prediction)\n",
    "    print(\"\\n{: >3.0f} \".format(0), end=\"\")\n",
    "    for r in range(classes):\n",
    "        prediction_img = prediction[0][:,:,r]*255/max_prob\n",
    "        cv2.imwrite(\"./deconv/img{}.jpg\".format(r), prediction_img)\n",
    "        '''\n",
    "        if r>0 and not r%10:\n",
    "            term = \"\\n{: >3.0f} \".format(int(r/10))\n",
    "        else:\n",
    "            term = \",\"\n",
    "        #print(\"{1: >6.2f}/{0: >4.2f}\".format(np.max(prediction[0][:,:,r]),np.sum(prediction[0][:,:,r])), end=term)\n",
    "        print(\"{0: >6.2f}\".format(np.max(prediction[0][:,:,r])/np.sum(prediction[0][:,:,r])), end=term)\n",
    "        '''\n",
    "\n",
    "def deconv_layer(inputs, \n",
    "                 filters, \n",
    "                 alpha=1.0, \n",
    "                 kernel=(3, 3), \n",
    "                 strides=(2, 2), \n",
    "                 block_id=1, \n",
    "                 activation=None, \n",
    "                 use_bias=True):\n",
    "    '''\n",
    "        Deconvolutional layers\n",
    "    '''\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    filters = int(filters * alpha)\n",
    "    x = Conv2DTranspose(filters, kernel,\n",
    "               padding='valid',\n",
    "               use_bias=use_bias,\n",
    "               strides=strides,\n",
    "               name='deconv_%d' % block_id,\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer='zeros')(inputs)\n",
    "    #x = BatchNormalization(axis=channel_axis, name='deconv_bn_%d' % block_id)(x)\n",
    "    if activation is None:\n",
    "        return Activation(relu, name='deconv_relu_%d' % block_id)(x)\n",
    "    else:\n",
    "        return activation(x)\n",
    "\n",
    "    \n",
    "def conv_layer(inputs, \n",
    "               filters, \n",
    "               alpha=1.0, \n",
    "               kernel=(3, 3), \n",
    "               strides=(1, 1), \n",
    "               block_id=1, \n",
    "               activation=None, \n",
    "               use_bias=False,\n",
    "               padding='valid'):\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    filters = int(filters * alpha)\n",
    "    x = Conv2D(filters, kernel,\n",
    "               padding=padding,\n",
    "               use_bias=use_bias,\n",
    "               strides=strides,\n",
    "               name='t_conv_%d' % block_id,\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer='zeros')(inputs)\n",
    "    x = BatchNormalization(axis=channel_axis, name='t_conv_bn_%d' % block_id)(x)\n",
    "    if activation is None:\n",
    "        return Activation(relu, name='t_conv_relu_%d' % block_id)(x)\n",
    "    else:\n",
    "        return activation(x)\n",
    "\n",
    "class CustomReshape(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(CustomReshape, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        super(CustomReshape, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.reshape(x, [-1, self.output_dim]) #np.reshape(x, (-1, 14)) \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        print(\"CustomReshape\", input_shape)\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim}\n",
    "        base_config = super(CustomReshape, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class BilinearUpSampling2D(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, scale_factor, **kwargs):\n",
    "        self.output_dim   = output_dim\n",
    "        self.scale_factor = scale_factor\n",
    "        super(BilinearUpSampling2D, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        super(BilinearUpSampling2D, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        input_size = tf.shape(x)\n",
    "        size = (self.scale_factor*input_size[1], self.scale_factor*input_size[2])\n",
    "        new_size = tf.convert_to_tensor(size, dtype=tf.int32)\n",
    "        return tf.image.resize_images(x, new_size, align_corners=True, method=tf.image.ResizeMethod.BILINEAR)\n",
    "        #return tf.reshape(x, [-1, self.output_dim]) \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        print(\"BilinearUpSampling2D\", input_shape)\n",
    "        return input_shape[:-1] + (self.output_dim,)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim, 'scale_factor': self.scale_factor}\n",
    "        base_config = super(BilinearUpSampling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def generate_magic_kernel(classes=15, channels=15):\n",
    "    magic_kernel = np.array([[0.25,0.75,0.75,0.25],\n",
    "                         [0.25,0.75,0.75,0.25],\n",
    "                         [0.25,0.75,0.75,0.25],\n",
    "                         [0.25,0.75,0.75,0.25]])\n",
    "    kernel = []\n",
    "    for c in range(channels):\n",
    "        kernel_channel\n",
    "        for cl in range(classes):\n",
    "            kernel_channel.append(magic_kernel)\n",
    "        kernel.append(kernel_channel)\n",
    "    return np.array(kernel)\n",
    "\n",
    "\n",
    "def preprocess_image(image, normalize=False, pad=0):\n",
    "    '''\n",
    "    Normalize is divide by 255.\n",
    "    pad is pad to multiples of 'pad'\n",
    "    '''\n",
    "    w = image.shape[1]\n",
    "    h = image.shape[0]\n",
    "    img = None\n",
    "    if pad > 0:\n",
    "        w_pad = pad - (w % pad)\n",
    "        h_pad = pad - (h % pad)\n",
    "        img = np.lib.pad(image, ((h_pad,0),(w_pad,0),(0,0)), 'constant', constant_values=(0,0))\n",
    "    if normalize:\n",
    "        img = img/255.\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_img(file, normalize=False, pad=0):\n",
    "    '''\n",
    "     Load images and always convert to RGB representation\n",
    "     If 'normalize' is True, then divide by 255.\n",
    "     If 'pad' is more than 0, pad the image to a multiple of the provided number\n",
    "    '''\n",
    "    image = Image.open(file)\n",
    "    if image.mode is not 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    return preprocess_image(np.asarray(image), normalize, pad)\n",
    "\n",
    "\n",
    "def pad_image(image, thickness=(1,1), padding=(1,1)):\n",
    "    return np.lib.pad(image, thickness, 'constant', constant_values=padding)\n",
    "\n",
    "\n",
    "def predict_with_model(image, model=None):\n",
    "    image_input = image\n",
    "    X = np.expand_dims(image_input, 0)\n",
    "    prediction = model.predict(X, len(X), True)\n",
    "    idx = np.argmax(prediction)\n",
    "    return idx, prediction\n",
    "\n",
    "\n",
    "def prediction_heatmaps(image, model, classes=2):\n",
    "    result = predict_with_model(image, model)\n",
    "    pred = result[1][0]\n",
    "    heatmap = np.concatenate([pad_image(pred[:,:,c], thickness=(1,1)) for c in range(classes)], axis=1)\n",
    "    return heatmap\n",
    "\n",
    "'''\n",
    "conv_model = Model(segmenter.input, segmenter.layers[-6].output)\n",
    "'''\n",
    "\n",
    "def extract_heatmap_class(heatmap, classid, classes=2):\n",
    "    frame_width = heatmap.shape[1]/classes\n",
    "    start = int(classid*frame_width)\n",
    "    end   = int(start + frame_width)\n",
    "    return heatmap[:,start:end]\n",
    "\n",
    "\n",
    "def image_clahe(image):\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    r = clahe.apply(image[:,:,0])\n",
    "    g = clahe.apply(image[:,:,1])\n",
    "    b = clahe.apply(image[:,:,2])\n",
    "    \n",
    "    return np.stack([r,g,b], -1)\n",
    "\n",
    "def process_image(img_file, fcn, conv, targets=[0,1], pad=0, equalize=False):\n",
    "    '''\n",
    "    Show heatmaps for image targets from deconvolutional model and pure fully convolutional model\n",
    "    targets is a list of class ids to show\n",
    "    '''\n",
    "    img = load_img(img_file, normalize=False, pad=pad)\n",
    "    if equalize:\n",
    "        img = image_clahe(img)\n",
    "        \n",
    "    print(\"Image Dimensions\", img.shape)\n",
    "    target_fcn = None\n",
    "    \n",
    "    if fcn is not None:\n",
    "        heatmap = prediction_heatmaps(img/255., fcn)\n",
    "        plt.figure(figsize=(15,2))\n",
    "        p = plt.imshow(heatmap, cmap=\"hot\")\n",
    "        print(\"Heatmap Dimensions\", heatmap.shape)\n",
    "        names = [class_names[n] for n in targets]\n",
    "        names.insert(0, \"Source\")\n",
    "        target_fcn = [extract_heatmap_class(heatmap, c) for c in targets]\n",
    "        target_fcn.insert(0, img)\n",
    "        show_grid(target_fcn, names, cmap_mono=\"hot\")\n",
    "\n",
    "    if conv is not None:\n",
    "        heatmap = prediction_heatmaps(img/255., conv)\n",
    "        plt.figure(figsize=(15,2))\n",
    "        p = plt.imshow(heatmap, cmap=\"hot\")\n",
    "        print(\"Heatmap Dimensions\", heatmap.shape)\n",
    "        names = [class_names[n] for n in targets]\n",
    "        names.insert(0, \"Source\")\n",
    "        target = [extract_heatmap_class(heatmap, c) for c in targets]\n",
    "        target.insert(0, img)\n",
    "        show_grid(target, names, cmap_mono=\"hot\")\n",
    "    return target_fcn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_skiplayer_fcn(source_model, classes=2, deconv=True, dropout=1e-3):\n",
    "    '''\n",
    "        Make Mobilenet fully convolutional with any size input and\n",
    "        segmentation output. source_model provides fully loaded Mobilenet \n",
    "        so we can extract its weights for the FCN model\n",
    "        \n",
    "        Return the fully convolutional model.\n",
    "    '''\n",
    "    x = source_model.layers[-1].output #Last layer used from Mobilenet. Choosing the last convolution\n",
    "    #x_channels = mobilenet.layers[-7].output_shape[-1]\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "\n",
    "    '''    \n",
    "    for i in range(len(source_model.layers) - 4):\n",
    "        source_model.layers[i].trainable = False\n",
    "    source_model.layers[-3].trainable = True\n",
    "    trainable_layers = 16 # Train the deconvolutions\n",
    "    '''\n",
    "\n",
    "    bias_flag = True\n",
    "    bias_init = 'he_normal'\n",
    "\n",
    "    skip_id = 14\n",
    "    skip_7 = source_model.layers[-skip_id].output\n",
    "    skip_7_channels = source_model.layers[-skip_id].output_shape[-1]\n",
    "    skip_7 = Conv2D(classes, (1,1),\n",
    "               padding=\"same\",\n",
    "               use_bias=bias_flag,\n",
    "               strides=(1,1),\n",
    "               activation='softmax',\n",
    "               name='t_skip_conv_7',\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer=bias_init)(skip_7)\n",
    "    \n",
    "    skip_id = 51\n",
    "    skip_5 = source_model.layers[-skip_id].output\n",
    "    skip_5_channels = source_model.layers[-skip_id].output_shape[-1]\n",
    "    skip_5 = Conv2D(classes, (1,1),\n",
    "               padding=\"same\",\n",
    "               use_bias=bias_flag,\n",
    "               strides=(1,1),\n",
    "               activation='softmax',\n",
    "               name='t_skip_conv_5',\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer=bias_init)(skip_5)\n",
    "    \n",
    "    skip_id = 64\n",
    "    skip_3 = source_model.layers[-skip_id].output\n",
    "    skip_3_channels = source_model.layers[-skip_id].output_shape[-1]\n",
    "    skip_3 = Conv2D(classes, (1,1),\n",
    "               padding=\"same\",\n",
    "               use_bias=bias_flag,\n",
    "               strides=(1,1),\n",
    "               activation='softmax',\n",
    "               name='t_skip_conv_3',\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer=bias_init)(skip_3)\n",
    "    \n",
    "    sig = Activation('softmax', name='act_softmax2')\n",
    "    bias_flag = True\n",
    "    bias_init = 'he_normal'\n",
    "    # Setup the network\n",
    "    x = Conv2D(classes, (1,1),\n",
    "               padding=\"same\",\n",
    "               use_bias=bias_flag,\n",
    "               strides=(1,1),\n",
    "               activation='relu',\n",
    "               name='t_reduce_channels_1',\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer=bias_init)(x)\n",
    "    x = BilinearUpSampling2D(scale_factor=2, output_dim=classes, name='upscore_1')(x)\n",
    "    x_skip7 = keras.layers.Add()([x, skip_7])\n",
    "    x_skip7 = BatchNormalization(axis=channel_axis, name='deconv_bn_%d' % 1)(x_skip7)\n",
    "    x = Conv2D(classes, (1,1),\n",
    "               padding=\"same\",\n",
    "               use_bias=bias_flag,\n",
    "               strides=(1,1),\n",
    "               activation='relu',\n",
    "               name='t_conv2d_smoothing_1',\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer=bias_init)(x_skip7)\n",
    "    x = BilinearUpSampling2D(scale_factor=2, output_dim=classes, name='upscore_2')(x)\n",
    "    x_skip5 = keras.layers.Add()([x, skip_5])\n",
    "    x_skip5 = BatchNormalization(axis=channel_axis, name='deconv_bn_%d' % 2)(x_skip5)\n",
    "    x = Conv2D(classes, (1,1),\n",
    "               padding=\"same\",\n",
    "               use_bias=bias_flag,\n",
    "               strides=(1,1),\n",
    "               activation='relu',\n",
    "               name='t_conv2d_smoothing_2',\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer=bias_init)(x_skip5)\n",
    "    x = BilinearUpSampling2D(scale_factor=2, output_dim=classes, name='upscore_3')(x)\n",
    "    x_skip3 = keras.layers.Add()([x, skip_3])\n",
    "    x_skip3 = BatchNormalization(axis=channel_axis, name='deconv_bn_%d' % 3)(x_skip3)\n",
    "    x = Conv2D(classes, (1,1),\n",
    "               padding=\"same\",\n",
    "               use_bias=bias_flag,\n",
    "               strides=(1,1),\n",
    "               activation='relu',\n",
    "               name='t_conv2d_smoothing_3',\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer=bias_init)(x_skip3)\n",
    "    x = BilinearUpSampling2D(scale_factor=2, output_dim=classes, name='upscore_4')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='deconv_bn_%d' % 4)(x)\n",
    "    x = Conv2D(classes, (1,1),\n",
    "               padding=\"same\",\n",
    "               use_bias=bias_flag,\n",
    "               strides=(1,1),\n",
    "               activation='relu',\n",
    "               name='t_conv2d_smoothing_4',\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer=bias_init)(x)\n",
    "    x = BilinearUpSampling2D(scale_factor=2, output_dim=classes, name='upscore_5')(x)\n",
    "    x = BatchNormalization(axis=channel_axis, name='deconv_bn_%d' % 5)(x)\n",
    "    x = Conv2D(classes, (1,1),\n",
    "               padding=\"same\",\n",
    "               use_bias=bias_flag,\n",
    "               strides=(1,1),\n",
    "               activation='softmax',\n",
    "               name='t_conv2d_smoothing_5',\n",
    "               kernel_initializer='he_normal',\n",
    "               bias_initializer=bias_init)(x)\n",
    "    #compose the combined model with the new top\n",
    "    new_model = Model(source_model.input, x)\n",
    "    for i in range(len(new_model.layers)-25):\n",
    "        new_model.layers[i].trainable = False\n",
    "\n",
    "    new_model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\n",
    "    new_model.summary()\n",
    "    return new_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def training_callbacks(folder=\"./training/\", name_prefix=\"classifier_\", batch=30, use_val=True):\n",
    "    '''\n",
    "    Generate callbacks to checkpoint model during training and reduce the learning rate on plateau\n",
    "    'use_val' decides whether to use training or validation statistics for decisions and reporting\n",
    "    '''\n",
    "    #filepath = \"./training/weights-improvement-{epoch:02d}-{val_acc:.3f}-{val_loss:.3f}.hdf5\"\n",
    "    filepath = folder+name_prefix+\"{epoch:02d}-{val_acc:.3f}-{val_loss:.3f}.hdf5\"\n",
    "    monitor_checkpoint = 'val_acc'\n",
    "    monitor_reduce_lr = 'val_loss'\n",
    "    if not use_val:\n",
    "        filepath = folder+name_prefix+\"{epoch:02d}-{acc:.3f}-{loss:.3f}.hdf5\"\n",
    "        monitor_checkpoint = 'acc'\n",
    "        monitor_reduce_lr = 'loss'\n",
    "    checkpoint = ModelCheckpoint(filepath, \n",
    "                                 monitor=monitor_checkpoint, \n",
    "                                 verbose=0, \n",
    "                                 save_best_only=True, \n",
    "                                 save_weights_only=False, \n",
    "                                 mode='auto')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor=monitor_reduce_lr, \n",
    "                                  factor=0.75, patience=5, \n",
    "                                  min_lr=0.000002, \n",
    "                                  mode='min', \n",
    "                                  verbose=0)\n",
    "    tensorboard = TensorBoard(log_dir='./tensorboard', \n",
    "                              histogram_freq=0, \n",
    "                              batch_size=batch, \n",
    "                              write_graph=True, \n",
    "                              write_grads=False, \n",
    "                              write_images=True, \n",
    "                              embeddings_freq=0, \n",
    "                              embeddings_layer_names=None, \n",
    "                              embeddings_metadata=None)\n",
    "    callbacks_list = [checkpoint, reduce_lr, tensorboard]\n",
    "    return callbacks_list\n",
    "\n",
    "\n",
    "\n",
    "def normalize_input(x):\n",
    "    #x = image_clahe(x.astype(\"uint8\"))\n",
    "    x = x / 127.5\n",
    "    x -= 1.\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import decode_traffic as dt\n",
    "\n",
    "def run():\n",
    "    #input_size = (256,256)\n",
    "    fcn_size   = (512,512)\n",
    "    classes    = 4\n",
    "    class_names = dt.TARGET_CLASS_NAMES\n",
    "\n",
    "    fcn_input = Input(shape=(None, None, 3))\n",
    "\n",
    "    mobilenetv1 = MobileNet(input_tensor=fcn_input, \n",
    "                            alpha=1.0, \n",
    "                            depth_multiplier=1, \n",
    "                            include_top=False, \n",
    "                            weights='imagenet', \n",
    "                            pooling=None, \n",
    "                            classes=1000)\n",
    "\n",
    "    fcn_model  = generate_skiplayer_fcn(mobilenetv1, \n",
    "                                        deconv=False, \n",
    "                                        classes=classes)\n",
    "\n",
    "    # we create two instances with the same arguments\n",
    "    shift = 0.2\n",
    "    data_format = K.image_data_format()\n",
    "    data_gen_args = dict(preprocessing_function=normalize_input,\n",
    "                         horizontal_flip=True,\n",
    "                         vertical_flip=True,\n",
    "                         height_shift_range=shift, \n",
    "                         width_shift_range=shift,\n",
    "                         brightness=5,\n",
    "                         fill_mode='constant',\n",
    "                         cval=0,\n",
    "                         data_format=data_format,\n",
    "                         num_classes=classes)\n",
    "    '''\n",
    "                         \n",
    "                         channel_shift_range=32,\n",
    "                         rotation_range=0)\n",
    "                         shear_range=0.2,\n",
    "                         zoom_range=0.2,\n",
    "    '''\n",
    "\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "    seed = 1\n",
    "    root = \"./data/\"\n",
    "    image_dir = root+\"input/\"\n",
    "    mask_dir  = root+\"output/\"\n",
    "    multiplier = 4\n",
    "    batch = 20\n",
    "    steps = 256 #Assume that the more variations to images, the more steps per epoch to cover them all\n",
    "\n",
    "    image_generator = image_datagen.flow_from_directory(image_dir,\n",
    "                                                        class_mode='sparse_mask',\n",
    "                                                        batch_size=batch,\n",
    "                                                        target_size=fcn_size)\n",
    "\n",
    "    fcn_model.fit_generator(image_generator,\n",
    "                            steps_per_epoch=steps,\n",
    "                            verbose=1,\n",
    "                            epochs=100,\n",
    "                            callbacks=training_callbacks(batch=batch, name_prefix=\"fcn_weights_\", use_val=False))\n",
    "    fcn_model.save('fcn_classifier_model.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Mobilenet Traffic Light FCN v3.0\")\n",
    "    print(\"Train Mobilenet FCN with 3 Skip Layers after data cleanup\")\n",
    "    run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
